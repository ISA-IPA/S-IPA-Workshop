Worked as BI developer to generate customer satisfaction (CSAT) BI reports for several front-line products like 
         Xbox, Zune and Office. Used SQL Server and MSBI tool stack 
         • Designed 
         Data Marts and Cubes and built automated ETL 
         data validation system for Enterprise 
         Data Warehouse 
         • Selected for Microsoft's leadership development program APEX for two years. Worked in 4 different teams as 
         Software Developer, Test Engineer and Program Manager roles and executed through full Software Development 
         Life Cycle (SDLC). Went through various early career leadership training boot-camps in USA and India 
          
         Representative Client Projects 
         1) Analytics foundations and road-map for a rising Private Bank in SEA 
         Problem Statement: The client wanted to explore how they could build and scale their analytics capabilities to drive 
         actionable insights for the Private Banking division. They engaged us to build two proof of concepts that would identify the existing gaps and also lay put the foundations of their Analytics road-map 
          
         Solution Approach: 
         • In this ~8 weeks engagement with the client we started with 
         data discovery process within the client's organization 
         (private bank + few other associated teams) and identified the gaps to achieve the "Analytics ready" 
         data. We 
         delivered sample ER diagram to build out the analytics 
         data lake and the associated 
         data governance and management needed to build it. 
         • Based on the available 
         data and client business priorities we shared a prioritized list of analytics use cases that the client could undertake to maximize ROE. 
         • By collaborating with multiple teams we closed lot of the critical 
         data gaps ourselves and executed on two use 
         cases as proof of concept: (i) 
         Data Driven Segmentation on their existing client base to generate actionable 
         insights for relationship managers and for the bank to optimize Net New Money (NNM) and Revenue over Asset 
          
         2 
         (RoA). (ii) External prospecting using a partner 
         data provider company who would help in identifying the right 
         prospective clients along with the latest events and news about them and also tag the most suitable relationship 
         manager for the client. 
         • Finally, we shared our point of views on how to operationalize these two POC use cases and also how to scale up (analytics operating model, 
         data lake, org design and immediate role requirements) the analytics capabilities 
          
         Business Impact: 
         • First POC resulted in a Tableau dashboard for Relationship Managers where they could see actionable insights 
         (generated by machine learning algorithms) for each of their accounts and also the relative performance of their 
         accounts with the "peer accounts" 
         • Second use case resulted in a web interface for Market Leads to identify the right prospects and assigning 
         suitable RM's to engage with the prospect. 
         • Both use cases were very well received by the Relationship Managers and Investment Advisors of the client 
         organization and a mandate was given to operationalize them. Also our work proved the concept to get top 
         leadership's buy in for additional analytical capabilities within the private bank 
          
         2) Modernize and upgrade customer acquisition platform for a major retail bank in India 
         Problem Statement: A major retail bank was trying to upgrade its digital marketing and customer acquisition framework to optimize marketing spend and maximize the conversion rate 
          
         Solution Approach: 
         • A robust 
         data architecture was designed to ingest various types of 
         data (clickstream, demographic etc.) to create 
         o a customer 360 view 
         • An AI based keyword prediction system was implemented to bid the right keywords in right time and at right place. 
         Then ML models were built to analyze clickstream 
         data in real time to categorize the leads by order of relevance 
         • Finally product propensity and channel propensities were computed to decide which product and channel would 
         be most relevant for the leads. A business rule engine was deployed based on the modeling outcome 
          
         Result: The solution doubled the digital conversion rate and reduced the annual digital marketing spend by 30% for the client in just two quarters of deployment 
          
         3) Modernizing 
         Data Architecture and Analytics road-map design for a global Business School in SEA 
         Problem Statement: The client wanted to upgrade their existing 
         data warehouse that was catering to their existing BI 
         reports and leverage modern datasets and tools to improve student experience and support faculty research 
          
         Solution Approach: 
         • After analyzing client's existing 
         data using a proprietary 
         data assessment framework, we came up with 43 distinct 
         
         data elements and grouped them into 5 
         data categories. We then overlaid those 
         data categories on the client's 
         existing IT applications and their hosting platforms. 
         • We recommended the client a list of target state applications and combination of public and private cloud based 
         hosting to consolidate the 
         data categories. 
         • We identified gaps in their existing 
         data warehouse and designed target state 
         data architecture with a 
         data lake and MDM and sequential steps to make transition to the target state. 
         • Also designed a target operating model, org structure and roles to build and scale up their analytics capabilities 
          
         Business Impact: The report helped the client create a case to launch their analytics COE and initiate the analytics use 
         cases which would help them maintain their global leadership in business academia 
          
         3 
         4) Optimize Eye Evaluation (EV testing) lanes for a major eye care facility in Singapore 
         Problem Statement: The client was having uneven workload across the EV lanes of various departments during different 
         part of the day causing the staffing/ resource planning difficult and also it was impacting patient wait times specially during rush hours. The client engaged us to identify how to optimize unit cost of service and patient wait time 
          
         Solution Approach: 
         • In this ~10 week engagement we created the process map for the patient journey to model all the interaction 
         touch-points. We created and validated assumptions on external factors driving that impacted the patient journey 
         • We created the baseline 
         data from historical 
         data and also from expert opinions/ experiences. Using those 
         data 
         we simulated the current state using AnyLogic tool and fine-tuned the modeling parameters by testing the model 
         outcomes with actual observations iteratively 
         • After finalizing the modeling parameter ranges, we simulated 17 future state scenarios and found that combining the EV lanes for some of the departments would generate optimal results 
         • To make sure the local optimizations would not negatively impact the overall patient experience we generated 
         recommendations based on two further analysis: 
         (i) Regression analysis to identify the key driving factors of overall patient wait time 
         (ii) A System Dynamics model encompassing the major patient touch points before and after the EV testing 
          
         Business Impact: This study helped the client to make a case for reorganizing the floor plans of the facility which would 
         potentially save them ~200K SGD per floor per year and also reduce the average patient wait time by 1-3 minutes 
          
         5) Develop an Anti Money Laundering (AML) proof of concept to reduce false positives for a major bank in USA 
         Problem Statement: The client engaged us to build a machine learning based AML detection POC that would reduce the high volume of false positives generated by their existing rule based AML systems 
          
         Solution Approach: 
         • We aggregated 
         data from multiple sources (KYC, payment/ transaction 
         data, external third party 
         data) to create a 
         0 
         customer 360 view. 
         • Using Named Entity Resolution (NER) and other NLP techniques we enhanced the 
         data quality 
         • After that we created a network graph with the transactions and identified fraudulent patterns by studying various 
         metrics e.g. Centrality, Between-ness, Shortest Paths and Strength of nodes (Page-rank score) 
         • We also identified potential money laundering rings by identifying the cluster of nodes with very high volume of transactions between themselves 
         • Finally we flagged the fraudulent transactions by observing the time activation patterns of the suspected 
         transaction cycle 
          
         Business Impact: The POC model had reduced the false positives by ~30% and we identified the additional 
         data sources which could potentially enhance the accuracy of the model even more after full operationalization 
          
         6) Develop product marketing strategy to increase customer equity for a online grocery store in USA 
         Problem Statement: Online grocery stores are focused on customer's ever changing needs. Modern generation's interest to eat healthy has posed one of the biggest e-commerce challenges and opportunities for manufacturers and retailers 
         alike. The client wanted a targeted marketing strategy to generate more revenue from their healthy products offerings. 
          
         Solution Approach: 
         The problem was broken down into four sub-problems and suitable models were developed: 
         1. The existing healthy shoppers were identified from previous years' order 
         data, customer demographic 
         data, 
         device 
         data, product catalogue etc. by applying RFM methodology on customer profile attributes 
         2. Shoppers were segmented based on healthy shopping persona by applying K-Means, Gaussian Mixture and LCA 
         3. To run up-sale and cross-sale promotions, "Healthy Baskets" were created with various healthy product 
         combinations for each healthy shopper segments (upgraders, switchers & deal-seekers) by Market Basket 
         Analysis, Discrete Choice Analysis and Conjoint Analysis 
          
         4 
         4. Healthy shopping persona was applied on the click-stream 
         data (from Google Analytics) to identify the appropriate 
         leads and products propensity. Those leads were over-laid on the previous campaign 
         data to identify the channel 
         propensity (email/ SMS/ mobile app based promotions etc.) 
         5. Estimated lift in Customer Lifetime Value (CLV) was calculated by migration model combining all the segments. 
          
         Finally, all the model outcomes were integrated into the business systems like Adobe Campaign Manager (ACM), Adobe 
         Audience Manager (AAM) and LiveRamp. For this project delivery SAS, R, SQL and Tableau were also used 
          
         Result: This targeted healthy product marketing strategy was estimated to generate additional customer equity of ~4 MM 
         USD over a span of 3 years. In the first year of implementing the model and running the targeted campaign, the customer 
         found a lift of ~1MM USD. Enhancement of Mobile app had a strong positive correlation with the sales lift 
          
         7) Develop an integrated Claims Analytics Engine for a health insurance company in USA 
         Problem Statement: The client wanted to build a claim analytics engine that would increase accuracy in processing, detect 
         frauds faster and enhance regulatory compliance. This engine was to be deployed during claim ingestion 
          
         Solution Approach: 
         • In this 9 months long engagement, we developed multiple machine learning models to predict the number of adjustments of the claims, claim processing time, probability of the claim attracting penalty (Prompt Pay) 
         • Segmentation and association rule mining based techniques were applied to detect fraudulent claims/ claim line 
         items within both in-state and inter-plan claims 
         • Finally all those model outcomes were converted into business rules and/or risk scores and applied to the claims 
         intake systems to flag appropriately. Suitable changes in the claims processing workflow was also operationalized 
          
         Business Impact: The unified claims analytics engine helped the client save ~8M dollars by avoiding prompt pay penalty and ~2M dollars by detecting fraudulent/ duplicate claims just within a year of deployment 
          
         8) Predict Natural Clinical Coding Gap Closure in Medicare Claims for a health insurance company in USA 
         Problem Statement: The client was spending lot of money to conduct manual medical chart review to close clinical coding 
         gaps in Medicare claims where clear diagnostic code is compulsory for CMS reimbursements. Many times the coding 
         gaps are closed "naturally" when the patient visits the doctor/ facility in near future in which case the intervention such as chart review is considered wasteful. The client was interested to reduce wasteful interventions, and go after the most 
         critical (high value, high risk) coding gaps. 
          
         Solution Approach: 
         The problem was broken down into multiple sub-problems and then suitable models were developed: 
         1. Coding gaps (persisting and suspected) were identified using association rule mining and event flow analysis 
         2. Provider classification was done based on their coding behavior using decision tree based model 
         3. Patient utilization model was built to predict future doctor visits/ readmissions. Various modeling techniques were 
         tried, e.g. KNN, Least Square Regression (OLS with LS Means), GLM, SVM, Ridge Regression, CART Bagging, and Monte Carlo Markov Chain (MCMC). However the best performing models were Gradient Boosting (GBM, 
         XGBoost), Artificial Neural Networks (ANN) and Random Forest 
         4. Intervention planning model was built based on past intervention dataset using Logistic Regression and LDA. 
          
         Finally, all the models were integrated together following a decision flow chart and represented in a user-friendly 
         dashboard. SQL, R and Tableau were used to implement various analytics components. Python based scripts were 
         developed to extract and integrate third party 
         data (IQVIA drug information, NLM databases) 
          
         Result: This novel approach helped the client save ~3 MM dollars in the first year alone after implementation. The model 
         had less than 20% misclassification error rate and to our knowledge it's the most advanced DIP model in the industry. A 
         white paper on this project was presented at RISE risk adjustment conference 
          
         5